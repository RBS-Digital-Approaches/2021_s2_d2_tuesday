{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s2_d2_m2b_mapping_marc_records_with_python.ipynb","provenance":[{"file_id":"1r0hEYDUql_5ef8g7eq2vV2sxNhJnyaqt","timestamp":1626184598270}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FMbhlhKfQIfe"},"source":["# Mapping MARC Records\n","In this notebook, we're going to use regularized place names to query a web API (in this case, the [GeoNames geographical database](https://www.geonames.org/) and retrieve latitude and longitude coordinates so that we can put those places on a map. \n","\n","In all of the screenshots, I'll be using a set of MARC records related to issues of John Bunyan's *The Pilgrim's Progress* prior to 1800 (which is available in the `data_class` folder. I chose those records, frankly, because I had them on hand and because I figured that there would be enough variety in the publication places to make it actually interesting to see on a map. If you have catalog records that you'd rather use, instead, please feel free. The *Pilgrim's Progress* records are MARC-21 records, but I've included brief asides about doing the same thing with MARCXML (which you could generalize to other kinds of XML) or with information in a .csv file.\n","\n","Because Monday's Python hands-on notebook spent so much time using MARC records handled with `Pymarc`, this notebook won't include nearly as much discussion of that sort. I'll provide comments in the code to explain what's going on, and we can talk about the approach I took here (and alternatives that occur to you) in our discussion period.\n","\n","We'll be using the Python `folium` package, which provides a wrapper for the popular [Leaflet JavaScript library](https://leafletjs.com/). This notebook is by no means a last word on this kind of mapping: \n","\n","* There are some limits to what we can do in a Colab notebook because of the way Colab does and does not work with some IPython extensions; \n","* `folium` doesn't seem to expose everything that `Leaflet.js` can do; \n","* Even if it did, my JavaScript knowledge is rusty enough that I'm not sure I could work out all the things we might want to do.\n","\n","This workbook focuses on getting the geocoding information we'd need to map our catalog records. The details of what to do next would vary depending on the specific software you're using, so I haven't pushed to figure out all the possible niceties of Leaflet maos, for example. Leaflet is very widely used, however, and people have made it possible to integrate Leaflet maps with lots of different platforms: if you imagine yourself doing more work with maps, it would probably be worth your while to investigate Leaflet further."]},{"cell_type":"markdown","metadata":{"id":"qGNWPDeUzWcG"},"source":["## If you get stuck\n","All the code in this notebook is shaped by what I encountered while working with the particular records I was working with when I wrote it. You may well need to adapt things if you're using different data. \n","\n","In the records I used in the section on MARCXML, for example, I had to build in a conditional to see if there was a publication city in the record—that never come up in the MARC-21 records for *The Pilgrim's Progress*. I also had to come up with some different ways of processing place names in those MARCXML records because most of the books were in languages other than English and handled place names differently. \n","\n","If you run into problems with your data, try to think about what control structures you would need to add to your code to catch and deal with the problems you encounter in your data. But you should always feel free to check in on Slack or Zoom if you get stuck!"]},{"cell_type":"markdown","metadata":{"id":"4hNE_0K-aWTS"},"source":["## Getting started\n","You'll use different packages depending on the format of your data, so I'll defer installing and importing packages until the relevant section. In the meantime, though, you'll need to connect to Google Drive and set a variable for the path to the directory that has your data."]},{"cell_type":"code","metadata":{"id":"L0dW4JEox0Jt"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ee0fqWtdodp9"},"source":["### Create a variable for our source directory\n","If you want to use one of the sets of catalog records we've provided, you can find them in the `data_class` directory that you've cloned from GitHub. If you have other records you'd like to use, instead, add them to the `rbs_digital_approaches_2021/data_my/` directory in your Google Drive and use that path, instead: comment out line 1 and uncomment line 2."]},{"cell_type":"code","metadata":{"id":"0gJTxrSfo7N8"},"source":["source_directory = '/gdrive/MyDrive/rbs_digital_approaches_2021/s2_data_class/'\n","# source_directory = '/gdrive/MyDrive/rbs_digital_approaches_2021/data_my/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvZTJl1BcO5x"},"source":["## Getting the official country codes used in MARC records\n","As we saw yesterday, the 008 field some defined data about the resource described in the MARC record. Yesterday we took advantage of the publication year available at characters 7-10. Today, we'll use the place of publication information in characters 15-17. This is a two- or three-character code drawn from an official list that's linked from the [MARC documentation for field 008](https://www.loc.gov/marc/countries/).\n","\n","I've used those codes as keys for a dictionary, with the corresponding country name as the value. We'll use this dictionary for decoding our country codes (`marc_country_codes['aa']` would return `'Albania'`, for example).\n","\n","The text editor that I use (BBEdit) has a very nice support for regular expressions in its find/replace dialogue. I created this dictionary by copying the text from the LOC's web site into a new text document in BBEdit, then did a single find-and replace. I looked for the following regular expression:\n","\n",">`^([\\-a-z]{2,4})\\s+(.+)`\n","\n","From left to right: \n","* `^`: A string at the beginning of a line (this wasn't strictly necessary)\n","* The parentheses create a \"capture group\": I don't just want to match what's inside the parentheses, I want to be able to reuse the texct that matches the pattern\n","* `[\\-a-z]{2,4}`: Any combination of between two and four hyphens or lower-case letters. The closed parenthesis marks the end of my capture pattern,\n","* `\\s+`: One or more whitespace characters\n","* `(.+)\\n`: Another capture group, this time consisting of one or more characters of any kind.\n","\n","I then replaced every instance of that regular expression using: \n","\n",">`'\\1': '\\2', `\n","\n","That is: Capture group 1 inside single quotes, followed by a colon and a space; then capture group 2 insinde single quotes, followed by a comma and a space. So, for example, \n","\n",">aa\tAlbania\n","\n","became\n","\n",">'aa': 'Albania',\n","\n","Then I copied the resulting text and pasted it into my Colab notebook cell between curly braces, and there was my dictionary.\n"]},{"cell_type":"code","metadata":{"id":"dada7mb2_nGk"},"source":["#Information from https://www.loc.gov/marc/countries/ Turned into a Python\n","#dictionary with a regular expression replacement in BBEdit\n","marc_country_codes = {\n","    \"aa\": \"Albania\",\n","\"abc\": \"Alberta\",\n","\"-ac\": \"Ashmore and Cartier Islands\",\n","\"aca\": \"Australian Capital Territory\",\n","\"ae\": \"Algeria\",\n","\"af\": \"Afghanistan\",\n","\"ag\": \"Argentina\",\n","\"-ai\": \"Anguilla\",\n","\"ai\": \"Armenia (Republic)\",\n","\"-air\": \"Armenian S.S.R.\",\n","\"aj\": \"Azerbaijan\",\n","\"-ajr\": \"Azerbaijan S.S.R.\",\n","\"aku\": \"Alaska\",\n","\"alu\": \"Alabama\",\n","\"am\": \"Anguilla\",\n","\"an\": \"Andorra\",\n","\"ao\": \"Angola\",\n","\"aq\": \"Antigua and Barbuda\",\n","\"aru\": \"Arkansas\",\n","\"as\": \"American Samoa\",\n","\"at\": \"Australia\",\n","\"au\": \"Austria\",\n","\"aw\": \"Aruba\",\n","\"ay\": \"Antarctica\",\n","\"azu\": \"Arizona\",\n","\"ba\": \"Bahrain\",\n","\"bb\": \"Barbados\",\n","\"bcc\": \"British Columbia\",\n","\"bd\": \"Burundi\",\n","\"be\": \"Belgium\",\n","\"bf\": \"Bahamas\",\n","\"bg\": \"Bangladesh\",\n","\"bh\": \"Belize\",\n","\"bi\": \"British Indian Ocean Territory\",\n","\"bl\": \"Brazil\",\n","\"bm\": \"Bermuda Islands\",\n","\"bn\": \"Bosnia and Herzegovina\",\n","\"bo\": \"Bolivia\",\n","\"bp\": \"Solomon Islands\",\n","\"br\": \"Burma\",\n","\"bs\": \"Botswana\",\n","\"bt\": \"Bhutan\",\n","\"bu\": \"Bulgaria\",\n","\"bv\": \"Bouvet Island\",\n","\"bw\": \"Belarus\",\n","\"-bwr\": \"Byelorussian S.S.R.\",\n","\"bx\": \"Brunei\",\n","\"ca\": \"Caribbean Netherlands\",\n","\"cau\": \"California\",\n","\"cb\": \"Cambodia\",\n","\"cc\": \"China\",\n","\"cd\": \"Chad\",\n","\"ce\": \"Sri Lanka\",\n","\"cf\": \"Congo (Brazzaville)\",\n","\"cg\": \"Congo (Democratic Republic)\",\n","\"ch\": \"China (Republic : 1949- )\",\n","\"ci\": \"Croatia\",\n","\"cj\": \"Cayman Islands\",\n","\"ck\": \"Colombia\",\n","\"cl\": \"Chile\",\n","\"cm\": \"Cameroon\",\n","\"-cn\": \"Canada\",\n","\"co\": \"Curaçao\",\n","\"cou\": \"Colorado\",\n","\"-cp\": \"Canton and Enderbury Islands\",\n","\"cq\": \"Comoros\",\n","\"cr\": \"Costa Rica\",\n","\"-cs\": \"Czechoslovakia\",\n","\"ctu\": \"Connecticut\",\n","\"cu\": \"Cuba\",\n","\"cv\": \"Cabo Verde\",\n","\"cw\": \"Cook Islands\",\n","\"cx\": \"Central African Republic\",\n","\"cy\": \"Cyprus\",\n","\"-cz\": \"Canal Zone\",\n","\"dcu\": \"District of Columbia\",\n","\"deu\": \"Delaware\",\n","\"dk\": \"Denmark\",\n","\"dm\": \"Benin\",\n","\"dq\": \"Dominica\",\n","\"dr\": \"Dominican Republic\",\n","\"ea\": \"Eritrea\",\n","\"ec\": \"Ecuador\",\n","\"eg\": \"Equatorial Guinea\",\n","\"em\": \"Timor-Leste\",\n","\"enk\": \"England\",\n","\"er\": \"Estonia\",\n","\"-err\": \"Estonia\",\n","\"es\": \"El Salvador\",\n","\"et\": \"Ethiopia\",\n","\"fa\": \"Faroe Islands\",\n","\"fg\": \"French Guiana\",\n","\"fi\": \"Finland\",\n","\"fj\": \"Fiji\",\n","\"fk\": \"Falkland Islands\",\n","\"flu\": \"Florida\",\n","\"fm\": \"Micronesia (Federated States)\",\n","\"fp\": \"French Polynesia\",\n","\"fr\": \"France\",\n","\"fs\": \"Terres australes et antarctiques françaises\",\n","\"ft\": \"Djibouti\",\n","\"gau\": \"Georgia\",\n","\"gb\": \"Kiribati\",\n","\"gd\": \"Grenada\",\n","\"-ge\": \"Germany (East)\",\n","\"gg\": \"Guernsey\",\n","\"gh\": \"Ghana\",\n","\"gi\": \"Gibraltar\",\n","\"gl\": \"Greenland\",\n","\"gm\": \"Gambia\",\n","\"-gn\": \"Gilbert and Ellice Islands\",\n","\"go\": \"Gabon\",\n","\"gp\": \"Guadeloupe\",\n","\"gr\": \"Greece\",\n","\"gs\": \"Georgia (Republic)\",\n","\"-gsr\": \"Georgian S.S.R.\",\n","\"gt\": \"Guatemala\",\n","\"gu\": \"Guam\",\n","\"gv\": \"Guinea\",\n","\"gw\": \"Germany\",\n","\"gy\": \"Guyana\",\n","\"gz\": \"Gaza Strip\",\n","\"hiu\": \"Hawaii\",\n","\"-hk\": \"Hong Kong\",\n","\"hm\": \"Heard and McDonald Islands\",\n","\"ho\": \"Honduras\",\n","\"ht\": \"Haiti\",\n","\"hu\": \"Hungary\",\n","\"iau\": \"Iowa\",\n","\"ic\": \"Iceland\",\n","\"idu\": \"Idaho\",\n","\"ie\": \"Ireland\",\n","\"ii\": \"India\",\n","\"ilu\": \"Illinois\",\n","\"im\": \"Isle of Man\",\n","\"inu\": \"Indiana\",\n","\"io\": \"Indonesia\",\n","\"iq\": \"Iraq\",\n","\"ir\": \"Iran\",\n","\"is\": \"Israel\",\n","\"it\": \"Italy\",\n","\"-iu\": \"Israel-Syria Demilitarized Zones\",\n","\"iv\": \"Côte d'Ivoire\",\n","\"-iw\": \"Israel-Jordan Demilitarized Zones\",\n","\"iy\": \"Iraq-Saudi Arabia Neutral Zone\",\n","\"ja\": \"Japan\",\n","\"je\": \"Jersey\",\n","\"ji\": \"Johnston Atoll\",\n","\"jm\": \"Jamaica\",\n","\"-jn\": \"Jan Mayen\",\n","\"jo\": \"Jordan\",\n","\"ke\": \"Kenya\",\n","\"kg\": \"Kyrgyzstan\",\n","\"-kgr\": \"Kirghiz S.S.R.\",\n","\"kn\": \"Korea (North)\",\n","\"ko\": \"Korea (South)\",\n","\"ksu\": \"Kansas\",\n","\"ku\": \"Kuwait\",\n","\"kv\": \"Kosovo\",\n","\"kyu\": \"Kentucky\",\n","\"kz\": \"Kazakhstan\",\n","\"-kzr\": \"Kazakh S.S.R.\",\n","\"lau\": \"Louisiana\",\n","\"lb\": \"Liberia\",\n","\"le\": \"Lebanon\",\n","\"lh\": \"Liechtenstein\",\n","\"li\": \"Lithuania\",\n","\"-lir\": \"Lithuania\",\n","\"-ln\": \"Central and Southern Line Islands\",\n","\"lo\": \"Lesotho\",\n","\"ls\": \"Laos\",\n","\"lu\": \"Luxembourg\",\n","\"lv\": \"Latvia\",\n","\"-lvr\": \"Latvia\",\n","\"ly\": \"Libya\",\n","\"mau\": \"Massachusetts\",\n","\"mbc\": \"Manitoba\",\n","\"mc\": \"Monaco\",\n","\"mdu\": \"Maryland\",\n","\"meu\": \"Maine\",\n","\"mf\": \"Mauritius\",\n","\"mg\": \"Madagascar\",\n","\"-mh\": \"Macao\",\n","\"miu\": \"Michigan\",\n","\"mj\": \"Montserrat\",\n","\"mk\": \"Oman\",\n","\"ml\": \"Mali\",\n","\"mm\": \"Malta\",\n","\"mnu\": \"Minnesota\",\n","\"mo\": \"Montenegro\",\n","\"mou\": \"Missouri\",\n","\"mp\": \"Mongolia\",\n","\"mq\": \"Martinique\",\n","\"mr\": \"Morocco\",\n","\"msu\": \"Mississippi\",\n","\"mtu\": \"Montana\",\n","\"mu\": \"Mauritania\",\n","\"mv\": \"Moldova\",\n","\"-mvr\": \"Moldavian S.S.R.\",\n","\"mw\": \"Malawi\",\n","\"mx\": \"Mexico\",\n","\"my\": \"Malaysia\",\n","\"mz\": \"Mozambique\",\n","\"-na\": \"Netherlands Antilles\",\n","\"nbu\": \"Nebraska\",\n","\"ncu\": \"North Carolina\",\n","\"ndu\": \"North Dakota\",\n","\"ne\": \"Netherlands\",\n","\"nfc\": \"Newfoundland and Labrador\",\n","\"ng\": \"Niger\",\n","\"nhu\": \"New Hampshire\",\n","\"nik\": \"Northern Ireland\",\n","\"nju\": \"New Jersey\",\n","\"nkc\": \"New Brunswick\",\n","\"nl\": \"New Caledonia\",\n","\"-nm\": \"Northern Mariana Islands\",\n","\"nmu\": \"New Mexico\",\n","\"nn\": \"Vanuatu\",\n","\"no\": \"Norway\",\n","\"np\": \"Nepal\",\n","\"nq\": \"Nicaragua\",\n","\"nr\": \"Nigeria\",\n","\"nsc\": \"Nova Scotia\",\n","\"ntc\": \"Northwest Territories\",\n","\"nu\": \"Nauru\",\n","\"nuc\": \"Nunavut\",\n","\"nvu\": \"Nevada\",\n","\"nw\": \"Northern Mariana Islands\",\n","\"nx\": \"Norfolk Island\",\n","\"nyu\": \"New York (State)\",\n","\"nz\": \"New Zealand\",\n","\"ohu\": \"Ohio\",\n","\"oku\": \"Oklahoma\",\n","\"onc\": \"Ontario\",\n","\"oru\": \"Oregon\",\n","\"ot\": \"Mayotte\",\n","\"pau\": \"Pennsylvania\",\n","\"pc\": \"Pitcairn Island\",\n","\"pe\": \"Peru\",\n","\"pf\": \"Paracel Islands\",\n","\"pg\": \"Guinea-Bissau\",\n","\"ph\": \"Philippines\",\n","\"pic\": \"Prince Edward Island\",\n","\"pk\": \"Pakistan\",\n","\"pl\": \"Poland\",\n","\"pn\": \"Panama\",\n","\"po\": \"Portugal\",\n","\"pp\": \"Papua New Guinea\",\n","\"pr\": \"Puerto Rico\",\n","\"-pt\": \"Portuguese Timor\",\n","\"pw\": \"Palau\",\n","\"py\": \"Paraguay\",\n","\"qa\": \"Qatar\",\n","\"qea\": \"Queensland\",\n","\"quc\": \"Québec (Province)\",\n","\"rb\": \"Serbia\",\n","\"re\": \"Réunion\",\n","\"rh\": \"Zimbabwe\",\n","\"riu\": \"Rhode Island\",\n","\"rm\": \"Romania\",\n","\"ru\": \"Russia (Federation)\",\n","\"-rur\": \"Russian S.F.S.R.\",\n","\"rw\": \"Rwanda\",\n","\"-ry\": \"Ryukyu Islands, Southern\",\n","\"sa\": \"South Africa\",\n","\"-sb\": \"Svalbard\",\n","\"sc\": \"Saint-Barthélemy\",\n","\"scu\": \"South Carolina\",\n","\"sd\": \"South Sudan\",\n","\"sdu\": \"South Dakota\",\n","\"se\": \"Seychelles\",\n","\"sf\": \"Sao Tome and Principe\",\n","\"sg\": \"Senegal\",\n","\"sh\": \"Spanish North Africa\",\n","\"si\": \"Singapore\",\n","\"sj\": \"Sudan\",\n","\"-sk\": \"Sikkim\",\n","\"sl\": \"Sierra Leone\",\n","\"sm\": \"San Marino\",\n","\"sn\": \"Sint Maarten\",\n","\"snc\": \"Saskatchewan\",\n","\"so\": \"Somalia\",\n","\"sp\": \"Spain\",\n","\"sq\": \"Eswatini\",\n","\"sr\": \"Surinam\",\n","\"ss\": \"Western Sahara\",\n","\"st\": \"Saint-Martin\",\n","\"stk\": \"Scotland\",\n","\"su\": \"Saudi Arabia\",\n","\"-sv\": \"Swan Islands\",\n","\"sw\": \"Sweden\",\n","\"sx\": \"Namibia\",\n","\"sy\": \"Syria\",\n","\"sz\": \"Switzerland\",\n","\"ta\": \"Tajikistan\",\n","\"-tar\": \"Tajik S.S.R.\",\n","\"tc\": \"Turks and Caicos Islands\",\n","\"tg\": \"Togo\",\n","\"th\": \"Thailand\",\n","\"ti\": \"Tunisia\",\n","\"tk\": \"Turkmenistan\",\n","\"-tkr\": \"Turkmen S.S.R.\",\n","\"tl\": \"Tokelau\",\n","\"tma\": \"Tasmania\",\n","\"tnu\": \"Tennessee\",\n","\"to\": \"Tonga\",\n","\"tr\": \"Trinidad and Tobago\",\n","\"ts\": \"United Arab Emirates\",\n","\"-tt\": \"Trust Territory of the Pacific Islands\",\n","\"tu\": \"Turkey\",\n","\"tv\": \"Tuvalu\",\n","\"txu\": \"Texas\",\n","\"tz\": \"Tanzania\",\n","\"ua\": \"Egypt\",\n","\"uc\": \"United States Misc. Caribbean Islands\",\n","\"ug\": \"Uganda\",\n","\"-ui\": \"United Kingdom Misc. Islands\",\n","\"-uik\": \"United Kingdom Misc. Islands\",\n","\"-uk\": \"United Kingdom\",\n","\"un\": \"Ukraine\",\n","\"-unr\": \"Ukraine\",\n","\"up\": \"United States Misc. Pacific Islands\",\n","\"-ur\": \"Soviet Union\",\n","\"-us\": \"United States\",\n","\"utu\": \"Utah\",\n","\"uv\": \"Burkina Faso\",\n","\"uy\": \"Uruguay\",\n","\"uz\": \"Uzbekistan\",\n","\"-uzr\": \"Uzbek S.S.R.\",\n","\"vau\": \"Virginia\",\n","\"vb\": \"British Virgin Islands\",\n","\"vc\": \"Vatican City\",\n","\"ve\": \"Venezuela\",\n","\"vi\": \"Virgin Islands of the United States\",\n","\"vm\": \"Vietnam\",\n","\"-vn\": \"Vietnam, North\",\n","\"vp\": \"Various places\",\n","\"vra\": \"Victoria\",\n","\"-vs\": \"Vietnam, South\",\n","\"vtu\": \"Vermont\",\n","\"wau\": \"Washington (State)\",\n","\"-wb\": \"West Berlin\",\n","\"wea\": \"Western Australia\",\n","\"wf\": \"Wallis and Futuna\",\n","\"wiu\": \"Wisconsin\",\n","\"wj\": \"West Bank of the Jordan River\",\n","\"wk\": \"Wake Island\",\n","\"wlk\": \"Wales\",\n","\"ws\": \"Samoa\",\n","\"wvu\": \"West Virginia\",\n","\"wyu\": \"Wyoming\",\n","\"xa\": \"Christmas Island (Indian Ocean)\",\n","\"xb\": \"Cocos (Keeling) Islands\",\n","\"xc\": \"Maldives\",\n","\"xd\": \"Saint Kitts-Nevis\",\n","\"xe\": \"Marshall Islands\",\n","\"xf\": \"Midway Islands\",\n","\"xga\": \"Coral Sea Islands Territory\",\n","\"xh\": \"Niue\",\n","\"-xi\": \"Saint Kitts-Nevis-Anguilla\",\n","\"xj\": \"Saint Helena\",\n","\"xk\": \"Saint Lucia\",\n","\"xl\": \"Saint Pierre and Miquelon\",\n","\"xm\": \"Saint Vincent and the Grenadines\",\n","\"xn\": \"North Macedonia\",\n","\"xna\": \"New South Wales\",\n","\"xo\": \"Slovakia\",\n","\"xoa\": \"Northern Territory\",\n","\"xp\": \"Spratly Island\",\n","\"xr\": \"Czech Republic\",\n","\"xra\": \"South Australia\",\n","\"xs\": \"South Georgia and the South Sandwich Islands\",\n","\"xv\": \"Slovenia\",\n","\"xx\": \"No place, unknown, or undetermined\",\n","\"xxc\": \"Canada\",\n","\"xxk\": \"United Kingdom\",\n","\"-xxr\": \"Soviet Union\",\n","\"xxu\": \"United States\",\n","\"ye\": \"Yemen\",\n","\"ykc\": \"Yukon Territory\",\n","\"-ys\": \"Yemen (People's Democratic Republic)\",\n","\"-yu\": \"Serbia and Montenegro\",\n","\"za\": \"Zambia\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"el2Fwc4Sh5fM"},"source":["## Creating Python data structures for the information from our records\n","Whatever format your records are in, we'll need structures for holding the information we extract from the records so that we can work with it later.\n","\n","I'm creating a dictionary to hold some basic information about each of my ESTC records, as well as a list for keeping track of distinct places. (This information doesn't need labels, in particular, so I'm using the simpler list structure.)"]},{"cell_type":"code","metadata":{"id":"uJdnTpoun4nN"},"source":["bib_records = {}\n","distinct_places = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2orsHYqiArm"},"source":["### Example 1: MARC-21 records\n","For records in MARC-21 format, I'll used `Pymarc`'s `MARCReader` module to read and parse the MARC records, just like we did yestersay. This will probably look pretty familiar, so I'll just provide some comments in the code, itself."]},{"cell_type":"code","metadata":{"id":"n9p3zCwfuzIh"},"source":["#We'll need Pymarc for reading MARC-21 records\n","!pip install pymarc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCmSIdMO2haL"},"source":["# Import the necessary components from the Pymarc library.\n","from pymarc import MARCReader\n","import re\n","\n","#Define a couple of regular expressions for stripping away punctuation that's\n","#included in the MARC fields\n","field_punctuation = re.compile(r'[\\s\\:]+$')\n","other_punctuation = re.compile(r'[\\[\\]\\?\\.,]')\n","with open(source_directory + '2021_s2_d2_estc_pilgrims_progress.mrc', 'rb') as infile :\n","    reader = MARCReader(infile) \n","    for record in reader :\n","      #Get the ESTC number from the 001 field\n","      estc_num = record['001'].data\n","      \n","      #If there is no key for that ESTC number in the records dictionary, create\n","      #one with an empty nested dictionary as the value\n","      bib_records.setdefault(estc_num, {})\n","      \n","      #Get the publication city from MARC field 260|a\n","      pub_city = record['260']['a']\n","      #Add it without modification to the nested dictionary for this record\n","      bib_records[estc_num]['original_260a'] = pub_city\n","      \n","      #Get rid of punctuation that's included in accord with cataloging rules\n","      #using the regular expression defined at line 7, above\n","      pub_city = re.sub(field_punctuation, '', pub_city)\n","      \n","      #If the publication city has \"i.e.\" in it\n","      if pub_city.find('i.e.') != -1 :\n","        #Only keep the string starting 5 characters ahead of the i in \"i.e.\"\n","        pub_city = pub_city[pub_city.find('i.e.')+5:]\n","      \n","      #Remove any other punctuation (like square brackets) from the publication \n","      #city, using the regular expression defined at line 8, above\n","      pub_city = re.sub(other_punctuation, '', pub_city)\n","      \n","      #Get the country code from MARC field 008, stripping any white space from \n","      #the right: some country codes are three characters long, others are only\n","      #two, and would bring white space with them\n","      country_code = record['008'].data[15:18].rstrip()\n","      \n","      #Get the value of the dictionary item whose key matches the country code\n","      country = marc_country_codes[country_code]\n","      \n","      #Add a tuple—an immutable list—consisting of the pub_city and full country\n","      #name to the nested dictionary for this record\n","      bib_records[estc_num]['place'] = (pub_city, country)\n","\n","      #If the tuple for the pairing of pub_city and country is not in our list \n","      #of distinct places, add it to that list\n","      if (pub_city, country) not in distinct_places :\n","        distinct_places.append((pub_city, country))\n","\n","#Sort the list of distinct_places alphabetically\n","distinct_places.sort()    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41oIbacSr74V"},"source":["### Example 2: MARCXML Records\n","I've downloaded a set of records related to Bartoloméo de las Casas (author of the *Brevísima relación de la destrucción de las Indias*) from the [Catálogo Colectivo del Patrimonio Bibliográfico](http://catalogos.mecd.es/CCPB/cgi-ccpb/abnetopac/) in MARCXML format. Because MARCXML is just an XML implementation of MARC, all the MARC field codes we've used will still apply. \n","\n","`Pymarc` can handle MARCXML records, but in this example I'll use the `BeautifulSoup` package along with `lxml` to show some approaches that are generally applicable to any XML data. (If you have records in a different flavor of XML, like MODS, you'll need to adapt the steps below to match your source.)"]},{"cell_type":"code","metadata":{"id":"DYRvySPytdUI"},"source":["#Import packages for working with XML\n","from bs4 import BeautifulSoup\n","import lxml"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubGjcCyTvDsP"},"source":["# Example permalink: http://catalogos.mecd.es/CCPB/cgi-ccpb/abnetopac?ACC=DOSEARCH&xsqf99=CCPB000413410-9\n","\n","#Define a couple of regular expressions for stripping away punctuation that's\n","#included in the MARC fields\n","field_punctuation = re.compile(r'[\\s\\:]+$')\n","other_punctuation = re.compile(r'[\\[\\]\\?\\.,]')\n","\n","with open(source_directory + '2021_s2_d2_CCPB_las_casas.xml', 'rb') as xml_file :\n","  xml_data = xml_file.read()\n","  soup = BeautifulSoup(xml_data, 'xml')\n","  records = soup.find_all('record')\n","  for record in records :\n","    field_001 = record.find('controlfield', tag = '001').get_text()\n","    bib_records.setdefault(field_001, {})\n","    if record.find('datafield', tag='260').find('subfield', code='a') is not None :\n","      pub_city = record.find('datafield', tag='260').find('subfield', code='a').get_text()\n","      pub_city = re.sub(field_punctuation, '', pub_city)\n","      # print(pub_city)\n","      \n","      #I don't typically work with records on resources that weren't published\n","      #in English-speaking countries, so I am *not at all* confident that these \n","      #next steps would hold up in all cases, but it seems to work for the records \n","      #I have.\n","      \n","      #If a cataloger placed a publication city in square brackets as the actual\n","      #place of publication, we should believe them.\n","      #I'm contructing a regular expression on the fly here searching for any\n","      #characters inside square brackets\n","      if re.search(r'\\[(.+)\\]', pub_city) is not None :\n","        #Taking advantage of the regular expression capture groups() function:\n","        #group(0) is the complete matched regular expression (e.g., [Barcelona]), \n","        #then there are as many groups as there are captured patterns. I only \n","        #have one capture group, so group(1) seems to be working: it turns\n","        #\"[Barcelona]\" into \"Barcelona\"\n","        pub_city = re.search(r'\\[(.+)\\]', pub_city).group(1)\n","\n","      #Get rid of everything but the last word, in hopes of excluding prepositions\n","      #(\"In\", \"A\", \"Tot\", etc.) and language about printing (\"Impresso en\", \n","      #\"Fue impressa ...\", etc.). This is potentially reckless on my part.\n","      pub_city = pub_city[pub_city.rfind(' ')+1:]\n","      # print(pub_city)\n","\n","    #Get the country code from MARC field 008, stripping any white space from \n","    #the right: some country codes are three characters long, others are only\n","    #two, and would bring white space with them\n","    field_008 = record.find('controlfield', tag='008').get_text()\n","    country_code = field_008[15:18].rstrip()\n","\n","    country = marc_country_codes[country_code]\n","\n","    bib_records[field_001]['place'] = (pub_city, country)\n","\n","    if (pub_city, country) not in distinct_places :\n","        distinct_places.append((pub_city, country))\n","\n","distinct_places.sort()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJjC9gGY6qj3"},"source":["### Example 3: Excel spreadsheet or .csv file\n","Some library catalogs allow export to Microsoft Excel or .csv, or you may very well have bibliographical records translated into one of those formats from some other source. \n","\n","I've provided a selection in both .csv and .xslx format of the first 148 records with a subject heading including \"Abolition\" published between 1740 and 1860 that are held by Harvard's Houghton Library.\n","\n","There's less we can do with the Excel and .csv files that Harbard's catalog is giving us here: there's a fair amount of information in there, but not the kind of granular cataloging data we've seen in MARC and MARCXML files: the publication city, imprint statement, and imprint year are simply concartenated into a sigle column, for instance, and we don't have any structured data field like MARC 008 to consult for things like a definitive statement of the publication country. We'll get the information we can—just the \n","\n","I generally work with data in .csv format using built-in `csv` library for Python, but the `pandas` package can handle both .csv and Microsoft Excel files, so let's use that instead."]},{"cell_type":"code","metadata":{"id":"pG8lhsbuBUvV"},"source":["# Example permalink: http://id.lib.harvard.edu/alma/990134930760203941/catalog\n","import pandas as pd\n","import re\n","data_file = '2021_s2_d2_houghton_abolition_1740_1860_selection.xlsx'\n","# data_file = '2021_s1_d2_houghton_abolition_1740_1860_selection.csv'\n","with open(source_directory + data_file, 'rb') as tabular_file :\n","  df_harvard_records = pd.read_excel(tabular_file)\n","  # df_harvard_records = pd.read_csv(tabular_file)\n","  \n","  #This seems to be how we iterate through rows in pandas...\n","  for index in df_harvard_records.index :\n","    #Get Harvard's catalog id to identify this record and construct a link\n","    #later\n","    record_id = df_harvard_records.loc[index]['HOLLIS number']\n","    bib_records.setdefault(record_id, {})\n","    \n","    #Get the \"Published\" column, which concatenates lots of information I wish\n","    #were separate\n","    publication_info = df_harvard_records.loc[index]['Published']\n","\n","    #Get the part of the \"Published\" column up to the colon (minus 1). It's just\n","    #a city, but that's all we're getting\n","    city_separator = re.compile(r'^([\\[\\w\\s\\-]+?)[\\.\\:,]')\n","    if re.search(city_separator, publication_info) is not None :\n","      pub_city = re.findall(city_separator, publication_info)[0].strip().lstrip('[')\n","\n","    #If we don't have this city yet, add it to our list of distinct_places\n","    if pub_city not in distinct_places :\n","      distinct_places.append(pub_city)\n","    \n","    #Update the nested dictionary for this record with a tuple consisting of\n","    #the pub_city and an empty string. We'll have to sort out countries in the \n","    #next step\n","    bib_records[record_id]['place'] = (pub_city, '')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9MI5BWmJxKs"},"source":["## Let's see what we have\n","Whichever example you followed above, you should now have a dictionary that uses some kind of record identifier for keys; the value for each key is a nested dictionary with a `place` key and a tuple (consisting of the publication city and country, if you were able to get it) as the value."]},{"cell_type":"code","metadata":{"id":"efhuHXs2D5aV"},"source":["for k, v in bib_records.items() :\n","  print(k, v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mA_g8riKR1S"},"source":["## Regularizing our place names\n","For the purposes of this exercise, I went with a pretty low-tech approach to regularizing the place names: I printed out all of the distinct place names in the distinct_place_names list, then copied and pasted them into a new plain text document in my rtext editor (BBEdit, in my case). \n","\n","Let's print those out so you can see how many places you're dealing with."]},{"cell_type":"code","metadata":{"id":"Rj4tW6dt99Qh"},"source":["#Print the list of distinct place names. Then copy them into a text editor\n","#and change them to a regularized form. You may have duplicate lines\n","print(len(distinct_places))\n","for distinct_place in distinct_places :\n","  print(distinct_place)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahv6-it9ND30"},"source":["Copy the output of the cell above and paste the distinct place names in your text editor. Then simply go through line by line and change the place name to a form that would have a good chance of returning a result from the GeoNames server. (In the case of ESTC records, for example, I had to change the Welsh name for Shrewsbury, England to \"Shrewsbury GB\".) \n","\n","Note that you may end up with duplicate lines (in the examples below, `('Boston', 'United States')`, `('Boston NE', 'United States')`, and `('Boston in New England', 'United States')` all get changed to `('Boston MA', 'United States')`, for instance.\n","\n","Also note that you shouldn't delete any lines: you want to have a replacement for every place name in your list.\n","\n","When you've come up with regularized forms for all your place names, paste them into the cell below, replacing the ESTC values in the `regularized_places` list.\n","\n","(Note the structure you want to end up with: `regularized_places` is a list that contains a series of tuples. \n","\n","* The `regularized_places` list, itself should open and close with square brackets.\n","* Each regularized place name in the `regularized_places` list is a tuple. For each tuple:\n","    - The tuple should be enclosed in parentheses\n","    - The name of each city should be in quotation marks\n","    - The name of each country should be in quotation marks\n","    - Each tuple should be separated by a comma from the one that follows"]},{"cell_type":"markdown","metadata":{"id":"fMLQKo0yM6yW"},"source":["\n","For the remainder of this walkthrough, I'm going to stick with just the ESTC records for *The Pilgrim's Progress* that I used in Example 1. If you're using different data, you just need to supply your own values for the `regularized_places` dictionary, below, keeping in mind the formatting that's described here."]},{"cell_type":"code","metadata":{"id":"Mj1S7uNxJZBm"},"source":["regularized_places = [('Shrewsbury', 'England'),\n","('Bath', 'England'),\n","('Birmingham', 'England'),\n","('Boston MA', 'United States'),\n","('Boston MA', 'United States'),\n","('Boston MA', 'United States'),\n","('Bristol', 'England'),\n","('Caerfyrddin', 'Wales'),\n","('Chester', 'England'),\n","('Coventry', 'England'),\n","('Dublin', 'Ireland'),\n","('Edinburgh', 'Scotland'),\n","('Edinburgh', 'Scotland'),\n","('Ephrata PA', 'United States'),\n","('Gainsborough', 'England'),\n","('Gainsborough', 'England'),\n","('Germantown PA', 'United States'),\n","('Germantown PA', 'United States'),\n","('Glasgow', 'Scotland'),\n","('Liverpool', 'England'),\n","('London', 'England'),\n","('Manchester', 'England'),\n","('New York', 'United States'),\n","('Newcastle upon Tyne', 'England'),\n","('Newcastle upon Tyne', 'England'),\n","('Nottingham', 'England'),\n","('Paisley', 'Scotland'),\n","('Philadelphia PA', 'United States'),\n","('Preston', 'England'),\n","('Shrewsbury', 'England'),\n","('Vepery', 'India'),\n","('Wolverhampton', 'England'),\n","('Worcester MA', 'United States'),\n","('Worcester MA', 'United States'),\n","('York', 'England')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SC0OlYyOcqP"},"source":["### Pairing up distinct place names from our records with their regularized forms\n","This next cell uses `zip` to create a pairwise relationship between the `distinct_places` list from our records and the `regularized_places` list that we created \"by hand.\"\n","\n","Our `regularization` variable creates a correspondence between each item in the `distinct_places` list and the item at the corresponding index in the `regularized_places` list (that's why it's important that there be one entry in `regularized_places` for every entry in `distinct_places`). \n","\n","The `for` loop here iterates through those pairings: `orig` represents the item in `distinct_places` and `reg` represents the corresponding item in `regularized_places`.\n","\n","As we iterate through those pairings, we *also* iterate through the entries in our `bib_records` dictionary (`record_id` is the key in our `bib_records` dictionary and `nested_dict` is the value for that key). If the value of `place` in the nested dictionary for a given `record_id` matches `orig` (the non-regularized version of the place name), we want to change that value to `reg` (the regularized form), instead.\n","\n","In the *Pilgrim's Progress* records, for example, on the first iteration through `regularized`, `orig` would equal `('Argraphwŷd yn y Mwŷthig', 'England')` (which is `distinct_places[0]`) and `reg` would equal `('Shrewsbury', 'England')` (which is `regularized_places[0]`).\n","\n","As we iterate through the `bib_records` dictionary, it's not until we reach `bib_records['R37515']` that we'll find that `bib_records['R37515']['place']` is equal to `orig`. But when we *do* find that match, we'll change the value of\n","`bib_records['R37515']['place']` from `('Argraphwŷd yn y Mwŷthig', 'England')` to `('Shrewsbury', 'England')`. "]},{"cell_type":"code","metadata":{"id":"WRcDJp1hLnHU"},"source":["regularization = zip(distinct_places, regularized_places)\n","for orig, reg in regularization :\n","  for record_id, nested_dict in bib_records.items() :\n","    if nested_dict['place'] == orig :\n","      nested_dict['place'] = reg\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmMbl9uWS0RD"},"source":["We can see our place names have now been updated to the version from `regularized_places`."]},{"cell_type":"code","metadata":{"id":"JX_jfj9UQzB6"},"source":["for k, v in bib_records.items() :\n","  print(k, v)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sp0QVvU_S_05"},"source":["### Getting ready to search for latitude/longitude coordinates\n","When we were regularizing our place names, we needed one regularized place name for every distinct place name. But, as we saw in the example of the *Pilgrim's Progress* records, that left us with three different instances of `('Boston MA', 'United States')`. We really only need to search the GeoNames server for Boston once. It's not like it's going anywhere.\n","\n","To get the unique values of our `refularized_places` list, we can use `set()`. But because we actually do want our `search_places` to end up as a list, let's turn that set into a list, in turn."]},{"cell_type":"code","metadata":{"id":"8jC_SV1xSB6Y"},"source":["search_places = list(set(regularized_places))\n","for search_place in sorted(search_places) :\n","  print(search_place)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCDIq27yTwCT"},"source":["## Actually searching for coordinates\n","Now that we have a list of uniquer place names that we want to search, we can finally construct URLs to send to the GeoNames API.\n","\n","We'll need the `re` (Regular Expressions) package to replace the spaces in our place names with the approapriate characters for URL encoding. (**Note:** There's probably a function in the `requests` module to handle this for us automatically, but it didn't occur to me to check...)\n","\n","We'll use the `requests` package for Python to handle sending and receiving our queries and the built-in `json` module for parsing the eresults. We'll also use the built-in `time` module to space out our requests by a couple of seconds, just to be polite."]},{"cell_type":"code","metadata":{"id":"VYFIOLwAEIxd"},"source":["import requests\n","import re\n","import json\n","import time\n","\n","#Create a dictionary to store the coordinates we'll be getting back\n","geocoded_places = {}\n","\n","#Iterate through our list of unique regularized place names\n","for search_place in sorted(search_places) :\n","  print(search_place)\n","  \n","  #Build up our query URL, starting with this base\n","  query_url = 'http://api.geonames.org/search?q='\n","  #Then adding the first item of our place name tuple (the city), replacing\n","  #any and all spaces with '%20', then putting another %20 ...\n","  query_url += re.sub(' ', '%20', search_place[0]) + '%20'\n","  #Adding the second part of our place name tuple, replacing spaces with '%20\n","  query_url += re.sub(' ', '%20', search_place[1])\n","  #Finally adding the end of our URL: be sure to replace <username> with\n","  # the class username\n","  query_url += '&maxRows=1&type=json&username=RBSDigitalApproaches'\n","  \n","  #Get the resulting URL using the requests module\n","  r = requests.get(query_url)\n","  #Parse the response from the GeoNames server as json\n","  response = r.json()\n","  \n","  #Create an entry in our geocoded_places with search_place as the key and\n","  #a nested dictionary as the value. The nested dictionary contains 'coordinates'\n","  #(a list of the the latitude and longitude that we get from the GeoNames json,\n","  #stored as a float, rather than a string), and a 'record_count' field set to 0\n","  geocoded_places.setdefault(search_place, \n","                             {'coordinates': \\\n","                              [float(response['geonames'][0]['lat']), \n","                               float(response['geonames'][0]['lng'])],\n","                              'record_count': 0})\n","  #Wait two seconds before sending another request\n","  time.sleep(2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1c97zglTWc8i"},"source":["Let's see what we've gotten back from GeoNames."]},{"cell_type":"code","metadata":{"id":"_Yp0hC_qWBxw"},"source":["for k, v in geocoded_places.items() :\n","  print(k, v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHaGkdFEWh3y"},"source":["### Figuring out the number of records for each place\n","Let's go back through our `bib_records` and figure out how many records we have for each of these places. \n","\n","For each entry in bib_records, we get the value of `place` in the nested dictionary and use that as the key in `geocoded_places` (because we regularized our place names, these should all match). Every time we encounter a record whose `place` is the same as the hey in our `geocdoded_palces` dictionary, we increase the value of `record_count` for that place by adding 1."]},{"cell_type":"code","metadata":{"id":"LopAtlkGYUNt"},"source":["for k, v in bib_records.items() :\n","  geocoded_places[v['place']]['record_count'] += 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85xgxYCgaGnw"},"source":["for k, v in geocoded_places.items() :\n","  print(k, v['record_count'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hn1YK5BcXlLb"},"source":["## Putting these places on the map\n","We'll us the `folium` package to create markers on a map: one marker for every geocoded place, with information about all the records associated with that place. (**Note:** This is only one approach, and not necessarily the best one, depending on what you want to do—it seemed like the easiest way, for demonstration purposes.)"]},{"cell_type":"code","metadata":{"id":"vw5QfkSMYFWA"},"source":["!pip install folium"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjM0PvlbYTJ6"},"source":["### Making our map with `folium`\n","I don't really want to get into too much detail about `folium`, specifically, because you may well find yourself using different software. I'll just point out a couple of things that are going on here, and then provide comments in the code.\n","\n","One slightly confusing thing that's happening in this cell (starting at line 5) is that I'm trying to synthesize coordinates that don't actually exist in my set of places so that I can figure out what rectangle will fit all of the coordinates I have into the initial view of the map: \n","\n","* I need a northeast corner that's as far north as the northernmost coordinate and as far east as the easternmost coordinate.\n","\n","* I need a southwest corner that's as far south as the southernmost coordinate and as far west as the westernmost coordinate.\n","\n","So I end up creating lists of latitudes and longitudes at lines 6 and 7 by iterating through all of the latitudes and all of the longitudes (the latitides are the first item in the lists of corrdinates for each place and the longitudes are the second item in that list). I'm using a more compact syntax than you've seen in any of our code before that plays to one of Python's strengths. This technique is called \"[list comprehension](https://www.python.org/dev/peps/pep-0202/).\"\n","\n","Then I use the first and last entries in each of those lists to create a northeast and a southwest point, respectively at lines 9 and 10. Those points end up being used as parameters for `fit_bounds()` at line 52. \n","\n","The other big thing that's happening here is building up a list of links back to the catalog to show when we hover over any of our markers. This is a nice idea, but the implementation could use some work: it's fine for Bath (1 record), but the legend for London (with 166 records) ends up way the heck up in the North Sea somewhere."]},{"cell_type":"code","metadata":{"id":"EYlqaLNKTVt8"},"source":["import folium\n","#Create a map\n","m = folium.Map(prefer_canvas=True, min_zoom=3)\n","\n","#Figure our northern-/southern-most and eastern-/western-most coordinates\n","lats = sorted([v['coordinates'][0] for k, v in geocoded_places.items()])\n","lngs = sorted([v['coordinates'][1] for k, v in geocoded_places.items()])\n","#Construct southwesternmost and northeasternmost points for fit_bounds()\n","sw = [lats[0], lngs[0]]\n","ne = [lats[-1], lngs[-1]]\n","\n","#Create text to show with markers\n","url_prefix = 'http://estc.bl.uk/'\n","#url_prefix = 'http://catalogos.mecd.es/CCPB/cgi-ccpb/abnetopac?ACC=DOSEARCH&xsqf99='\n","#url_prefix = 'http://id.lib.harvard.edu/alma/'\n","\n","#Iterate through unique places\n","for k, v in geocoded_places.items() :\n","  #Construct an empty list to hold links back to teh catalog\n","  catalog_links = []\n","\n","  #Iterate thrlugh the bib_records dictionary\n","  for record_id, record_contents in bib_records.items() :\n","    if record_contents['place'] == k :\n","      #Construct a link to teh catalog record and add it to the list\n","      catalog_links.append('<a href=\"' + url_prefix + record_id + '\">' + record_id + '</a>')\n","      #For Harvard:\n","      # catalog_links.append('<a href=\"' + url_prefix + record_id + '/catalog\">' + record_id + '</a>')\n","    \n","    #If there's only one record, use singular \"record\" at line 38, below, if \n","    #there are more, use plural \"records\"\n","    if v['record_count'] == 1 :\n","        record_form =  ' record:\\n'\n","    else :\n","        record_form = ' records\\n'\n","    \n","    #Create a legend to display when we hover over a marker\n","    legend = k[0] + '\\n' + str(v['record_count']) + record_form\n","    #Join together all of our catalog links and add them to the legend\n","    legend += '\\n'.join(catalog_links)\n","  \n","  #Construct a marker for each place. Locate it at the coordinates for that place\n","  #and use the legend created at lines 38 and 40 as a popup. Then add the marker\n","  #to the map object we created at line 3\n","  folium.Marker(\n","      location = v['coordinates'],\n","      popup = legend).add_to(m)\n","\n","#Zoom the map so that all markers are visible within an initial zoom level \n","#defined by the southwesternmost and northeasternmost corners we defined at\n","#lines 9 and 10\n","m.fit_bounds([sw, ne])\n","#Show the map\n","m"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjAu8Ot4fRFD"},"source":["## Conclusion\n","This is a fairly rough-and-ready example of mapping, but hopefully it's enough to introduce a few key ideas:\n","\n","* When we repurpose data, there's almost always going to be cleaning and regularization that need to happen. As this notebook shows, that may involve a mix of code and \"manual\" work. \n","* We can use data we have to get data that somebody else has using an API (here, the GeoNames API)\n","* Once we have our data in a form that allows us to draw in data from elsewhere, lots of interesting possibilities open up.\n","\n","If we were using a different environment (and had more time), there are all sorts of things we might do differently with this map. Hopefully, this is enough to give us a jumping off point for discussing other kinds of things we might do along these lines."]}]}